{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c24d5d-9973-45a1-83be-8bca8b03e576",
   "metadata": {},
   "source": [
    "# Assignment: Programming Review\n",
    "## Do Q1 and one other question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fb7b5-0345-447d-840a-59f667fe9c0c",
   "metadata": {},
   "source": [
    "**Q1.** First, think about your priorities in life. What kind of salary do you want to make after graduation? Do you mind getting more schooling? What kind of work-life balance are you looking for? Where do you want to work, geographically? You don't have to write this down here, just think about it.\n",
    "\n",
    "1. Go to the Occupational Outlook Handbook at [https://www.bls.gov/ooh/](https://www.bls.gov/ooh/). Look up \"Data Scientist.\" Read about the job and start collecting data about it from the job profile (e.g. salary, education required, work setting)\n",
    "2. Find 7-10 other jobs that appeal to you, and collect the same data as you did for Data Scientist. Put it all in a spreadsheet.\n",
    "3. Do any of your findings surprise you?\n",
    "4. Rank the jobs you picked from best to worst, and briefly explain why you did so.\n",
    "5. Please submit your spreadsheet with the assignment --- you can \"de-identify\" it and remove anything that you find personally identifying or you don't want to share, of course. We'll play with these data later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d65ad-3740-43d3-a944-b3653fbeb80c",
   "metadata": {},
   "source": [
    "3. I decided to compare all the occupations listed under Computer and Information Technology Occupations. One thing that surprised me was the coexistence of a \"Computer Programmer\" job and a \"Software Developer\" job. I don't really see the distinction between these two roles, but the number of programmers is expected to decline in the next ten years while the number of developers (including QA and testers) is projected to increase, which doesn't make much sense.\n",
    "4. I primarily based these rankings off a comparison between annual salary and percentage forecasted growth over ten years, with an occupation that scores highly in both being ranked near the top. From there, I considered the required education and experience levels, as well as the number of jobs in that occupation.\n",
    "   1. Software Developers, Quality Assurance Analysts, and Testers -- second-highest salary and relatively high growth. Additionally, it's a huge field with only a bachelor's typically required and no work experience prerequisite.\n",
    "   2. Computer and Information Research Scientists -- have the highest average salary and third-highest growth, though it's worth noting that it's a very small field and requires the most advanced education.\n",
    "   3. Computer Network Architects -- comparable salary and growth to software developers, but requires more job experience, making it a career choice for established professionals.\n",
    "   4. Information Security Analysts -- pays relatively well and has the second-most projected growth, but does require a little job experience.\n",
    "   5. Data Scientists -- solid pay, really high growth.\n",
    "   6. Database Administrators and Architects -- pays fairly well, but middling growth in a smaller field with more requirements involved as far as job experience.\n",
    "   7. Computer Systems Analysts -- pay is OK relative to field; growth isn't amazing as a percentage but it's already a big field so there should be plenty of jobs available.\n",
    "   8. Web Developers and Digital Designers -- some of the lowest pay among these occupations but at least growth is only mediocre rather than negative.\n",
    "   9. Network and Computer Systems Administrators -- mediocre pay and jobs expected to decrease (albeit by not a *huge* amount).\n",
    "   10. Computer Programmers -- 10% decrease in jobs is pretty bad.\n",
    "   11. Computer Support Specialists -- by far the lowest pay and not amazing growth either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa75b2-aaef-4368-a043-93437887879c",
   "metadata": {},
   "source": [
    "**Q2.** Being able to make basic plots to visualize sets of points is really helpful, particularly with data analysis. The pyplot plots are built up slowly by defining elements of the plot, and then using `plt.show()` to create the final plot. This question gives you some practice doing that **iterative** building process.\n",
    "\n",
    "1. Import the `numpy` module as `np` and the `matplotlib.pyplot` package as `plt`.\n",
    "2. Use `np.linspace` to create a grid of 50 points ranging from 0 to 1.\n",
    "3. Create a numpy array $y$ containing the values for the natural logarithm function using the `np.log(x)` function. Create a numpy array $z$ containing the values for the exponential function using the `np.exp(x)` function.\n",
    "4. Use the `plt.scatter(x,y)` method for the $y$ and $z$ vectors to create two scatter plots of the points you've created.\n",
    "5. Use the `plt.show()` method to create the plot.\n",
    "\n",
    "That plot has some problems.\n",
    "\n",
    "6. Before the `plt.show()` call, add labels to the $x$ and $y$ axes using `plt.xlabel(label)` and `plt.ylabel(label)`. Add a title to the graph using `plt.title(title)`, like \"Natural Log and Exponential Functions\".\n",
    "7. That looks a lot better, but we need a legend. When you screate the scatter plots, add the argument `label='Natural Log'` or `label='Exponential'` to the `.scatter` method call. Before the `plt.show()` call, add a `plt.legend(loc = 'lower right')` method call, which creates a legend in the lower right.\n",
    "\n",
    "Now do it again, with slightly less direction:\n",
    "\n",
    "8. Create a grid of 100 equally spaced points ranging from -6.5 to 6.5.\n",
    "9. Use the sine and cosine functions in Numpy to compute the values of those functions for each point on your grid. (You'll have to find out what those functions are.)\n",
    "10. Plot the values of the two functions for the values on the grid on the same plot.\n",
    "11. The scatter plot looks really noisy. Instead of `plt.scatter(x,y)` to make a scatter plot, use `plt.plot(x,y)` to make a line graph.\n",
    "12. Make the plot again, with labels for the axes, a title, and a legend in the lower left instead of the lower right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c05ee86-06dc-4ae9-896d-0cb70c4d6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57312f-fd41-4763-b38c-3b7c1b062b1c",
   "metadata": {},
   "source": [
    "**Q3.** This is a basic review of some statistics along with practice writing functions. Like we talked about, Python is a general purpose programming language and ships without basic data handling or statistical packages coded in. The beginning of the code chunk below generates random values for you to test your work on; since the random values are generates as NumPy arrays, you'll need to use the Numpy methods `np.sum(x)` to sum the vector $x$ and `np.sqrt(x)` to take the square roots of the values in $x$, as well as the Python function `len(x)` to get the length of $x$.\n",
    "\n",
    "Try to reuse the functions you've already defined as you work through the following questions, rather than rewriting code you've already written.\n",
    "\n",
    "1. Write a function that computes the **sample average** or **mean** of a vector $x$,\n",
    "$$\n",
    "\\bar{x} = \\dfrac{x_1 + x_2 + ... + x_N}{N} = \\dfrac{\\sum_{i=1}^N x_i}{N}.\n",
    "$$\n",
    "Write a function in the code chunk below to compute this quantity, and then use it to compute the mean of $x$.\n",
    "2. Write a function that computes the **sample standard deviation** of a vector $x$,\n",
    "$$\n",
    "s_x = \\sqrt{\\dfrac{(x_1 - \\bar{x})^2 + ... (x_N - \\bar{x})^2 }{N-1}} = \\sqrt{\\dfrac{ \\sum_{i=1}^N (x_i - \\bar{x})^2 }{N-1}}.\n",
    "$$\n",
    "The intuition of this quantity is that it computes roughly the average distance from each point $x_i$ to the sample mean $\\bar{x}$. If it is small, it means all the points are clustered tightly around the mean, and if it is large, it means the points are typically far away from the average. Write a function in the code chunk below to compute this quantity, and then use it to compute the sample standard deviation of $x$.\n",
    "3. Write a function that calls the previous two to **standardize** the values of the vector as a **$z$-score**:\n",
    "$$\n",
    "z = \\dfrac{x-\\bar{x}}{s}.\n",
    "$$\n",
    "The intuition of this quantity is that it is recentering all the values of $x$ so the average is zero and then scaling them by the standard deviation. If the data are normally distributed and $N$ is large, the $z$ score will approximately follow a standard normal distribution. Write a function in the code chunk below to compute this quantity, and then use it to compute the z-scores for $x$.\n",
    "4. The **sample covariance** of two vectors $x=(x_1,...,x_N)$ and $y=(y_1,...,y_N)$ is defined as\n",
    "$$\n",
    "cov(x,y) = \\dfrac{(x_1 - \\bar{x})(y_1-\\bar{y}) + (x_2 - \\bar{x})(y_2-\\bar{y}) + ... + (x_N - \\bar{x})(y_{N}-\\bar{y})}{N-1}\n",
    "$$\n",
    "$$\n",
    "= \\dfrac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{N-1}.\n",
    "$$\n",
    "The intuition of this quantity is that it looks at the pairs $(x_i, y_i)$ and compares them to the means $(\\bar{x},\\bar{y})$ to determine whether $x$ and $y$ tend to co-vary in the same direction relative to their means: If the values of $x$ and $y$ are typically both above or below the mean values of $x$ and $y$, then $x$ and $y$ will have a positive covariance, but if $x$ is typically above the mean of $x$ when $y$ is typically below the mean of $y$ or vice versa, then they will have a negative covariance. Write a function in the code chunk below to compute this quantity, and then use it to compute the covariance of the generated $x$ and $y$.\n",
    "6. The **sample correlation coefficient** of two vectors $x$ and $y$ is defined as\n",
    "$$\n",
    "r_{xy} = \\dfrac{cov(x,y)}{s_x s_y}\n",
    "$$\n",
    "Use your functions to create a new function that compute this quantity. The intuition of this quantity is that it is like the covariance, but normalized so that its values like between -1 and 1: perfect negative linear association between the variables at -1, no association at 0, and perfect positive linear association between the variables at 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f42ab788-af0c-47a4-a103-46c9a37bfad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean of x: -1.0899806430403438\n",
      "Sample standard deviation of x: 3.2847981697718236\n",
      "Z-scores of x: [np.float64(1.6552974299794228), np.float64(-1.0506096717069326), np.float64(-0.7457640270720693), np.float64(-0.38160531691525745), np.float64(0.24681908060018573), np.float64(0.5218992178856622), np.float64(0.7094022802514282), np.float64(-0.5930657550271107), np.float64(0.7048959221202706), np.float64(0.1966797065821503), np.float64(-1.1109441694497977), np.float64(0.0852210437519531), np.float64(0.048356702283393095), np.float64(-0.3548647462613931), np.float64(0.3292843671938974), np.float64(-0.4565562035834983), np.float64(0.8633534567088365), np.float64(-0.7319441660563886), np.float64(-1.3678925970592895), np.float64(0.9928086549934853), np.float64(0.38055771150184553), np.float64(-0.10044905472587069), np.float64(-1.4744878132365848), np.float64(0.974866812665865), np.float64(1.7571487154960421), np.float64(0.9778073204015572), np.float64(0.701655901634546), np.float64(0.03084328534941634), np.float64(1.043458678454602), np.float64(-0.07773298593058116), np.float64(0.8172866392910364), np.float64(-0.181985598324363), np.float64(1.278934798943009), np.float64(-0.37346853074306013), np.float64(-1.1716789016352502), np.float64(1.7414434442468898), np.float64(0.18678422788499205), np.float64(-1.8956509305893636), np.float64(-0.9004013569065047), np.float64(-0.7088624781254029), np.float64(-0.4519558158651541), np.float64(1.603362816348893), np.float64(-0.5822375159868036), np.float64(-0.06407808692007864), np.float64(0.30605910775495554), np.float64(1.6214756519329119), np.float64(-1.7203461113754004), np.float64(-1.6053491414684906), np.float64(0.0959582963661928), np.float64(-0.2999379956363676), np.float64(1.3223433335547103), np.float64(2.6849717273495584), np.float64(0.1601956297551476), np.float64(-0.08628165058593337), np.float64(-0.4044969014858263), np.float64(1.202076722309917), np.float64(0.17042931866443706), np.float64(0.10165924132105655), np.float64(-1.2495796187476118), np.float64(1.746520930621761), np.float64(1.8061051017139582), np.float64(1.0557804030263782), np.float64(-0.042631271932581156), np.float64(-0.8225713693811583), np.float64(-0.6198550880122857), np.float64(1.1264396239481222), np.float64(-0.9398579190983539), np.float64(1.081559130831479), np.float64(1.392897601077237), np.float64(-0.5536288144664682), np.float64(1.1622066929178243), np.float64(-0.7544338466904551), np.float64(-1.8904255802067533), np.float64(0.7282336785252533), np.float64(-0.21358171141630877), np.float64(-2.054768610564979), np.float64(-0.33491645579127904), np.float64(0.08537646270812745), np.float64(-1.6344814730992772), np.float64(-0.9931581722395214), np.float64(-0.07218951580454736), np.float64(-0.962591398055458), np.float64(-1.9916985725893734), np.float64(0.09917395308881445), np.float64(1.8083753957478033), np.float64(-0.574516695585191), np.float64(-0.30645707732319216), np.float64(0.5338528596595399), np.float64(-0.4421428615825611), np.float64(0.12923965268591775), np.float64(0.44328927332651263), np.float64(-0.5657444072418532), np.float64(-1.1210048705383517), np.float64(0.5383942786012564), np.float64(-0.5434336023358922), np.float64(-1.4545144135339387), np.float64(-0.604992581080238), np.float64(0.5623818714948534), np.float64(0.3845765561962104), np.float64(0.8743077256925548), np.float64(-0.691036906542246), np.float64(-0.5869593780297937), np.float64(0.19561495819029012), np.float64(-1.5532718552677798), np.float64(-0.2968379615855777), np.float64(-0.5168917238841663), np.float64(-0.680975101662073), np.float64(-0.5000584135034453), np.float64(-0.3817317193732792), np.float64(-0.749877821217997), np.float64(1.3034524736356523), np.float64(-0.2568379734163348), np.float64(0.02300192897123055), np.float64(0.5627523888170315), np.float64(-1.0168973242611314), np.float64(0.955520653332699), np.float64(0.36346133044672047), np.float64(-0.17057771994950352), np.float64(-0.792649133719601), np.float64(1.9816408452736156), np.float64(0.41443762225946834), np.float64(0.5318714130441863), np.float64(-0.7660751841557003), np.float64(-0.8527954893694357), np.float64(-1.3655048718571896), np.float64(0.45388108391044596), np.float64(-1.2224469257282171), np.float64(0.22556376696345243), np.float64(-2.208684094975654), np.float64(0.43032908727781494), np.float64(-0.7055220359493859), np.float64(-1.1997692830157796), np.float64(1.2857847110478096), np.float64(0.21384963667849283), np.float64(0.6687804041562252), np.float64(0.3053018816525035), np.float64(-0.3972819994587513), np.float64(-0.6844391413377027), np.float64(2.0793499593285114), np.float64(0.3767470498066506), np.float64(0.14632695445970212), np.float64(-1.6248724204506475), np.float64(0.3921692582573669), np.float64(0.28547405147912225), np.float64(0.09580780985106756), np.float64(-1.5791581487921262), np.float64(-0.8858796877272636), np.float64(-0.30319304555995163), np.float64(-0.26711836038146963), np.float64(1.740671258218967), np.float64(0.5060061741642476), np.float64(-2.1420290973126086), np.float64(-1.2016754046977756), np.float64(-1.6837027191436449), np.float64(-0.6306256802503846), np.float64(-0.469870625415814), np.float64(-0.050603388513611715), np.float64(-1.5251168988209702), np.float64(0.22358042256524546), np.float64(0.3221911638932175), np.float64(-0.746501276935458), np.float64(1.2555602718757621), np.float64(-1.1315481137529722), np.float64(1.0900264395184065), np.float64(-0.3114241101932946), np.float64(0.38028545717288004), np.float64(0.9347619206777716), np.float64(0.8975316399608337), np.float64(0.30807626306868297), np.float64(-2.7894199798588337), np.float64(-0.34638816685104795), np.float64(-0.58372064741032), np.float64(0.17372724534112488), np.float64(0.11350870127662598), np.float64(1.660479679266517), np.float64(1.0603333571345723), np.float64(-0.340329313240609), np.float64(-0.7886095058705865), np.float64(-1.3806067773866368), np.float64(-0.392061706451305), np.float64(0.24097817055152596), np.float64(0.10912740722950821), np.float64(-0.46124040331137467), np.float64(0.49474171560560154), np.float64(1.9797274462229868), np.float64(1.3445572163859654), np.float64(1.7542181849875895), np.float64(1.1467490611990052), np.float64(1.0809670072624478), np.float64(-0.29486175063120956), np.float64(-0.7203837134844335), np.float64(1.4446059659260122), np.float64(1.8290552928091175), np.float64(0.42010532777444554), np.float64(0.5689950334975312), np.float64(0.14842147549000195), np.float64(0.2997077455247992), np.float64(-0.017774145797184986), np.float64(2.009109162061164), np.float64(0.0006866855481871784)]\n",
      "Sample covariance of x and y: -2.8396505928938773\n",
      "Sample correlation coefficient of x and y: -0.42205093921264875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math as math\n",
    "np.random.seed(100) # Set the seed for the random number generator\n",
    "rho, sigma_x, sigma_y = -.4, 3, 2 # Variance-Covariance Parameters\n",
    "vcv = np.array([[sigma_x**2, rho*sigma_x*sigma_y],\n",
    "                [rho*sigma_x*sigma_y,sigma_y**2]]) # VCV Matrix\n",
    "mu = np.array([-1,2]) # Population averages\n",
    "sample = np.random.multivariate_normal(mu,vcv,200) # Multivariate normal draws\n",
    "x = sample[:,0]\n",
    "y = sample[:,1]\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "def sample_mean(x):\n",
    "    return np.sum(x) / len(x)\n",
    "\n",
    "def sample_stdev(x):\n",
    "    sm = sample_mean(x)\n",
    "    v = np.array([(z - sm) ** 2 for z in x])\n",
    "    var = sample_mean(v) * len(x) / (len(x) - 1)\n",
    "    return var ** 0.5\n",
    "\n",
    "def z_score(x, sm, sstdev):\n",
    "    return (x - sm) / sstdev\n",
    "\n",
    "def sample_covariance(x, y):\n",
    "    smx = sample_mean(x)\n",
    "    smy = sample_mean(y)\n",
    "    vx = np.array([(z - smx) for z in x])\n",
    "    vy = np.array([(z - smy) for z in y])\n",
    "    v = np.array([vx[i] * vy[i] for i in range(len(x))]) \n",
    "    return sample_mean(v) * len(x) / (len(x) - 1)\n",
    "\n",
    "def sample_correlation_coefficient(x, y):\n",
    "    return sample_covariance(x, y) / (sample_stdev(x) * sample_stdev(y))\n",
    "\n",
    "print(\"Sample mean of x:\", sample_mean(x))\n",
    "print(\"Sample standard deviation of x:\", sample_stdev(x))\n",
    "print(\"Z-scores of x:\", [z_score(z, sample_mean(x), sample_stdev(x)) for z in x])\n",
    "print(\"Sample covariance of x and y:\", sample_covariance(x, y))\n",
    "print(\"Sample correlation coefficient of x and y:\", sample_correlation_coefficient(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924279e-4b74-4941-9819-f028ce3db974",
   "metadata": {},
   "source": [
    "**Q4.** Optimization is at the core of data science and statistics: Picking the best fit or estimate according to some desirable criteria (e.g. Maximum Likelihood Estimation). In this question, you're going to write a function that finds the highest possible value for some function, and returns the value of the function and the maximizing value.\n",
    "\n",
    "1. Write a function that creates a grid from some value $a$ to a second value $b$ with $N$ steps, computes the value of a function $f$ on that grid, and then finds the maximizers of the function. Have it return a dictionary with the maximum value and the maximizers themselves.\n",
    "2. Use your function to maximize $f(x) = 100 - 2x^2 + 3x$ for values of $a=-1$, $b=1$, and for values of $N=3$, $N=10$, $N=100$, $N=1000$ and $N=5000$.\n",
    "3. The true maximizer of this function is $.75$ and the true maximum value is $101.125$. Is that what you got in the previous step? Why not? How does the quality of the maximization depend on $N$? How does your computed answer change as $N$ gets larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b32977-7687-4770-8f12-2ab0593f2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e5e34-00c0-4101-a1af-34cf61d69ffc",
   "metadata": {},
   "source": [
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
